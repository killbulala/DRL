# -*- coding: utf-8 -*-
"""
@Time ï¼š 2023/4/28 11:37
@Auth ï¼š killbulala
@File ï¼š 07_sarsa.py
@IDE  ï¼š PyCharm
policy iteration
    policy gradient
value iteration
    sarsa
    q-learning
Rt : immediate reward
Gt : Rt+1 + Î³Rt+2 + Î³^2Rt+3 + '''   total reward
bellman equation è´å°”æ›¼æ–¹ç¨‹
    state value function  çŠ¶æ€ä»·å€¼å‡½æ•°
    action value function åŠ¨ä½œä»·å€¼å‡½æ•°
MDP: markov decision process
é©¬å°”å¯å¤«æ€§ : ä¸‹ä¸€æ—¶åˆ»çš„çŠ¶æ€ä¸ä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€ç›¸å…³,ä¸ä¹‹å‰çš„çŠ¶æ€æ— å…³(bellman equationæˆç«‹çš„å‰ææ¡ä»¶)
ğ‘„ğœ‹(ğ‘ ,ğ‘)) : çŠ¶æ€sä¸‹å‘ç”ŸaåŠ¨ä½œçš„ä»·å€¼å‡½æ•°
    å»ºç«‹åˆå§‹Q-table
    é€šè¿‡sarsaç®—æ³•è¿­ä»£æ›´æ–°Q-table

epsilon-greedy (explore, exploit)
    é€šè¿‡éšæœºæ•°ä¸epsilonå…³ç³»ç¡®å®šæ˜¯æ¢ç´¢è¿˜æ˜¯åˆ©ç”¨
    å­¦ä¹ åˆæœŸå°½é‡å¤šé€‰æ‹©exploreæ¨¡å¼, éšä¹‹å­¦ä¹ è¿›è¡Œè¦æ›´å…³æ³¨exploitæ¨¡å¼
"""
import numpy as np
import gym
import matplotlib.pyplot as plt

"""maze env"""
fig = plt.figure(figsize=(5, 5))
ax = plt.gca()
ax.set_xlim(0, 3)
ax.set_ylim(0, 3)

plt.plot([2, 3], [1, 1], color='red', linewidth=2)
plt.plot([0, 1], [1, 1], color='red', linewidth=2)
plt.plot([1, 1], [1, 2], color='red', linewidth=2)
plt.plot([1, 2], [2, 2], color='red', linewidth=2)

plt.text(0.5, 2.5, 'S0', size=14, ha='center')
plt.text(1.5, 2.5, 'S1', size=14, ha='center')
plt.text(2.5, 2.5, 'S2', size=14, ha='center')
plt.text(0.5, 1.5, 'S3', size=14, ha='center')
plt.text(1.5, 1.5, 'S4', size=14, ha='center')
plt.text(2.5, 1.5, 'S5', size=14, ha='center')
plt.text(0.5, 0.5, 'S6', size=14, ha='center')
plt.text(1.5, 0.5, 'S7', size=14, ha='center')
plt.text(2.5, 0.5, 'S8', size=14, ha='center')
plt.text(0.5, 2.3, 'START', ha='center')
plt.text(2.5, 0.3, 'GOAL', ha='center')
# plt.axis('off')
plt.tick_params(axis='both', which='both',
                bottom=False, top=False,
                right=False, left=False,
                labelbottom=False, labelleft=False
                )
line, = ax.plot([0.5], [2.5], marker='o', color='g', markersize=60)

"""maze"""


class MazeEnv(gym.Env):
    def __init__(self):
        self.state = 0

    def reset(self):
        self.state = 0
        return self.state

    def step(self, action):
        if action == 0:
            self.state -= 3
        elif action == 1:
            self.state += 1
        elif action == 2:
            self.state += 3
        elif action == 3:
            self.state -= 1
        done = False
        reward = 0
        if self.state == 8:
            done = True
            reward = 1
        # state, reward, done, _
        return self.state, reward, done, {}


"""agent"""


class Agent:
    def __init__(self):
        self.action_space = list(range(4))
        self.theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0
                                 [np.nan, 1, np.nan, 1],  # s1
                                 [np.nan, np.nan, 1, 1],  # s2
                                 [1, np.nan, np.nan, np.nan],  # s3
                                 [np.nan, 1, 1, np.nan],  # s4
                                 [1, np.nan, np.nan, 1],  # s5
                                 [np.nan, 1, np.nan, np.nan],  # s6
                                 [1, 1, np.nan, 1]]  # s7
                                )
        self.pi = self._cvt_theta_to_pi()
        self.Q = np.random.rand(*self.theta_0.shape) * self.theta_0
        self.eta = 0.1
        self.gamma = 0.9
        self.eps = 0.5

    def _cvt_theta_to_pi(self):
        m, n = self.theta_0.shape
        pi = np.zeros((m, n))
        for r in range(m):
            pi[r, :] = self.theta_0[r, :] / np.nansum(self.theta_0[r, :])
        return np.nan_to_num(pi)

    def choose_action(self, s):
        # eps, explore  epsçš„æ¦‚ç‡è¿›è¡Œå¼€å‘æ¢ç´¢
        if np.random.rand() < self.eps:  # np.random.rand() è¿”å›0-1å‡åŒ€åˆ†å¸ƒéšæœºæ ·æœ¬
            action = np.random.choice(self.action_space, p=self.pi[s, :])
        else:
            # 1-eps, exploit  1-epsçš„æ¦‚ç‡è¿›è¡Œåˆ©ç”¨
            action = np.nanargmax(self.Q[s, :])
        return action

    def sarsa(self, s, a, r, s_next, a_next):
        """
        ç†æƒ³æƒ…å†µï¼š
            ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)=ğ‘…ğ‘¡+1+ğ›¾ğ‘„(ğ‘ ğ‘¡+1,ğ‘ğ‘¡+1)
        åå·®ä¼°è®¡td(temporal difference error):
            td = ğ‘…ğ‘¡+1+ğ›¾ğ‘„(ğ‘ ğ‘¡+1,ğ‘ğ‘¡+1) - ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)
        final update equation:
            ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)=ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)+ğœ‚â‹…[ğ‘…ğ‘¡+1+ğ›¾ğ‘„(ğ‘ ğ‘¡+1,ğ‘ğ‘¡+1)âˆ’ğ‘„(ğ‘ ğ‘¡,ğ‘ğ‘¡)]
            ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘Ÿğ‘¡+1,ğ‘ ğ‘¡+1,ğ‘ğ‘¡+1   -> sarsa
        æŠ˜æ‰£ï¼ˆdiscount factorï¼Œ ğ›¾ï¼‰: æŠ˜æ‰£å› å­ æœ‰åŠ©äºç¼©çŸ­æ­¥æ•°,ä¹Ÿå¯ä»¥æ›´å¿«çš„ç»“æŸä»»åŠ¡
        :param s:
        :param a:
        :param r:
        :param s_next:
        :param a_next:
        :return:
        """
        if s_next == 8:
            self.Q[s, a] = self.Q[s, a] + self.eta * (r - self.Q[s, a])
        else:
            self.Q[s, a] = self.Q[s, a] + self.eta * (r + self.gamma * self.Q[s_next, a_next] - self.Q[s, a])


"""Sarsa (update  ğ‘„ğœ‹(ğ‘ ,ğ‘))"""
maze = MazeEnv()
agent = Agent()
epoch = 0
threshold = 1e-5
while True:
    # ä¿å­˜ä¸Šä¸€ä¸ªQ-table
    pre_Q = np.nanmax(agent.Q, axis=1)
    state = maze.reset()
    action = agent.choose_action(state)
    trajectory = [[state, np.nan]]
    while True:
        trajectory[-1][1] = action
        state_next, reward, done, info = maze.step(action)
        trajectory.append([state_next, np.nan])
        if done:
            action_next = np.nan
        else:
            action_next = agent.choose_action(state_next)
        agent.sarsa(state, action, reward, state_next, action_next)
        if done:
            break
        else:
            action = action_next
            state = state_next
    # trajectory, agent.Q-table è®¡ç®—ä¸¤æ¬¡åå·®
    td = np.sum(np.abs(np.nanmax(agent.Q, axis=1) - pre_Q))
    epoch += 1
    # éšç€å­¦ä¹ è¿›è¡Œ å°†epsiloné™ä½ ä½¿å¾—æ›´åŠ å…³æ³¨åˆ©ç”¨æ¨¡å¼
    agent.eps /= 2
    print(epoch, td, len(trajectory))
    # æ ¹æ®å­¦ä¹ è½®æ•°å’Œåå·®å¤§å°ç»“æŸå­¦ä¹ 
    if epoch > 50 or td < threshold:
        break
